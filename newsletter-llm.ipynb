{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13463276,"datasetId":8499860,"databundleVersionId":14181147},{"sourceType":"modelInstanceVersion","sourceId":331777,"databundleVersionId":11790928,"modelInstanceId":269134}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"nvidiaTeslaT4"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport json\nimport pandas as pd\nimport gspread\nfrom kaggle_secrets import UserSecretsClient\nfrom datetime import datetime, timedelta, date\nimport sys\n\nprint(\"Authenticating with Google...\")\ntry:\n    user_secrets = UserSecretsClient()\n    google_creds_json = user_secrets.get_secret(\"GOOGLE_SHEETS_CREDENTIALS\")\n    google_creds_dict = json.loads(google_creds_json)\n    gc = gspread.service_account_from_dict(google_creds_dict)\n    print(\"Authentication successful!\")\nexcept Exception as e:\n    print(f\"Authentication failed: {e}\")\n    raise\n\nspreadsheet = gc.open_by_url(\n    'https://docs.google.com/spreadsheets/d/1yekpw2BcsAGxU--PDlPUDl4W2BKyBKH2XBYpmkqp_bI/edit#gid=0'\n)\n\n# Force a fresh pull of spreadsheet metadata (from the server)\nmetadata = spreadsheet.fetch_sheet_metadata()\n\n# You can inspect it if you want:\nprint(\"Spreadsheet title:\", metadata[\"properties\"][\"title\"])\nprint(\"Last modified time:\", metadata[\"properties\"].get(\"modifiedTime\", \"unknown\"))\n\n# Then explicitly reload your worksheet and its contents:\nworksheet = spreadsheet.get_worksheet(0)\nrows = worksheet.get_all_values()  # <-- this always gets the *live* data\nprint(f\"‚úÖ Loaded {len(rows)} rows from live sheet\")\nprint(\"Sample last row:\", rows[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:02.689241Z","iopub.execute_input":"2025-11-03T09:05:02.689460Z","iopub.status.idle":"2025-11-03T09:05:08.494899Z","shell.execute_reply.started":"2025-11-03T09:05:02.689438Z","shell.execute_reply":"2025-11-03T09:05:08.494284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Get subjects woorksheet ---\n\nworksheet = spreadsheet.get_worksheet(0)\ndata = worksheet.get_all_records()\ndf = pd.DataFrame(data)\n\nif 'Date max' in df.columns and 'Traited' in df.columns:\n    df['Date max'] = pd.to_datetime(df['Date max'], format='%d/%m/%Y')\n    df['Traited'] = df['Traited'].astype(str).str.strip().str.upper() == 'TRUE'\n    today = pd.to_datetime('today').normalize()\n\n    # The 'Traited' column is now a proper boolean type.\n    # Filter the DataFrame based on the conditions:\n    # 1. 'Date max' is in the future\n    # 2. 'Traited' is False\n    filtered_df = df[(df['Date max'] > today) | (df['Traited'] == False)]\n\n    print(\"\\nFiltered data (where 'Date max' > today OR 'Traited' is False):\")\n    if not filtered_df.empty:\n        print(filtered_df)\n    else:\n        print(\"No rows match the specified criteria.\")\n        print(\"Stopping Kaggle notebook execution.\")\n        sys.exit()\n\n\n\n# This cell assumes 'gspread' is imported, as gspread.Cell is used.\n# If not already imported, add: import gspread\n\nprint(\"\\nChecking for overdue and untraited items to update in Google Sheet...\")\n\n# 1. Define the condition: 'Date max' < today AND 'Traited' == False\ncondition = (df['Date max'] < today) & (df['Traited'] == False)\n\n# 2. Get the subset of the DataFrame that matches this condition\nrows_to_update = df[condition]\n\nif not rows_to_update.empty:\n    print(f\"Found {len(rows_to_update)} overdue item(s) to mark as 'TRUE'.\")\n    \n    try:\n        # 3. Get the column number for 'Traited'\n        # We add 1 because gspread columns are 1-indexed, while df columns are 0-indexed\n        traited_col_index = df.columns.get_loc('Traited') + 1\n        \n        # 4. Prepare a list of cells for batch update\n        cells_to_update = []\n        for df_index in rows_to_update.index:\n            # df.index 0 corresponds to sheet row 2 (after header)\n            sheet_row = int(df_index) + 2 \n            \n            # Create a gspread.Cell object with the row, col, and new value\n            cells_to_update.append(gspread.Cell(sheet_row, traited_col_index, True))\n        \n        # 5. Perform the batch update in the Google Sheet\n        if cells_to_update:\n            worksheet.update_cells(cells_to_update)\n            print(f\"Successfully updated {len(cells_to_update)} cells in the Google Sheet.\")\n            \n            # 6. (Optional) Update the local DataFrame to reflect this change\n            df.loc[condition, 'Traited'] = True\n            print(\"Local DataFrame 'df' has been updated to match.\")\n\n    except ValueError:\n        print(\"Error: 'Traited' column not found in the DataFrame.\")\n    except Exception as e:\n        print(f\"An error occurred during the Google Sheet update: {e}\")\n\nelse:\n    print(\"No overdue and untraited rows found. No updates were needed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:08.495567Z","iopub.execute_input":"2025-11-03T09:05:08.495903Z","iopub.status.idle":"2025-11-03T09:05:09.937046Z","shell.execute_reply.started":"2025-11-03T09:05:08.495885Z","shell.execute_reply":"2025-11-03T09:05:09.936351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert a (filtered) Google Sheet DataFrame into the `subjects` JSON you specified.\n# Fix: month number now correctly written in dates (no more \"m\" placeholders).\n\nimport pandas as pd\nimport re\nimport json\nfrom datetime import datetime\n\n# ---------- Helpers ----------\ndef canonize(col: str) -> str:\n    \"\"\"Lowercase, collapse internal whitespace to single space, strip ends.\"\"\"\n    return re.sub(r\"\\s+\", \" \", str(col)).strip().lower()\n\ndef to_iso_z(val):\n    \"\"\"Convert many date representations to 'YYYY-MM-DDT00:00:00Z' or return None.\"\"\"\n    if val is None:\n        return None\n    if isinstance(val, pd.Timestamp):\n        if pd.isna(val):\n            return None\n        return val.strftime(\"%Y-%m-%dT00:00:00Z\")\n\n    s = str(val).strip()\n    if not s or s.lower() in {\"nan\", \"nat\"}:\n        return None\n\n    # Handle keyword TODAY\n    if s.lower() == \"today\":\n        return datetime.now().strftime(\"%Y-%m-%dT00:00:00Z\")\n\n    # Handle numeric or string month-day-year issues (replace accidental 'm' placeholders)\n    s = re.sub(r\"[^0-9/\\-]\", \"\", s)\n\n    # Try common explicit formats\n    for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%d-%m-%Y\", \"%Y/%m/%d\"):\n        try:\n            dt = datetime.strptime(s, fmt)\n            return dt.strftime(\"%Y-%m-%dT00:00:00Z\")\n        except ValueError:\n            continue\n\n    # Fallback to pandas parser (dayfirst=True for European dates)\n    dt = pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n    if pd.isna(dt):\n        return None\n    return pd.Timestamp(dt).strftime(\"%Y-%m-%dT00:00:00Z\")\n\ndef safe_int(val, default=0):\n    \"\"\"Safely convert a value to an integer, returning a default on failure.\"\"\"\n    try:\n        if pd.isna(val):\n            return default\n    except Exception:\n        pass\n    try:\n        return int(str(val).strip())\n    except (ValueError, TypeError):\n        try:\n            return int(float(val))\n        except (ValueError, TypeError):\n            return default\n\ndef normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Create a copy of the DataFrame with canonical header names.\"\"\"\n    df = df.copy()\n    df.columns = [canonize(c) for c in df.columns]\n    return df\n\n# ---------- Acquire base_df ----------\nif \"filtered_df\" in globals():\n    base_df = filtered_df.copy()\nelse:\n    if \"df\" not in globals():\n        raise RuntimeError(\"Neither 'filtered_df' nor 'df' exist. Run the previous cell that builds them.\")\n    df_norm = normalize_headers(df)\n    if \"date max\" not in df_norm.columns or \"traited\" not in df_norm.columns:\n        raise RuntimeError(\"Expected columns 'Date max' and 'Traited' not found (even after header normalization).\")\n\n    try:\n        df_norm[\"date max\"] = pd.to_datetime(df_norm[\"date max\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n    except Exception:\n        df_norm[\"date max\"] = pd.to_datetime(df_norm[\"date max\"], errors=\"coerce\", dayfirst=True)\n\n    df_norm[\"traited\"] = (\n        df_norm[\"traited\"]\n        .astype(str)\n        .str.strip()\n        .str.upper()\n        .isin([\"TRUE\", \"1\", \"YES\", \"Y\", \"T\"])\n    )\n\n    today = pd.to_datetime(\"today\").normalize()\n    mask = (df_norm[\"date max\"] > today) | (df_norm[\"traited\"] == False)\n    base_df = df_norm.loc[mask].copy()\n\n# ---------- Build subjects JSON ----------\nbase_df = normalize_headers(base_df)\ncanon_cols = list(base_df.columns)\n\ncategory_cols = [c for c in canon_cols if c.startswith(\"category\")]\ncategory_cols = sorted(category_cols, key=lambda c: int(re.sub(r\"\\D\", \"\", c) or 0))\n\nsubjects = []\nskipped_rows_log = []\n\nfor index, row in base_df.iterrows():\n    topic = str(row.get(\"subject description\", \"\")).strip()\n    if not topic:\n        skipped_rows_log.append(f\"GSheet Row {index + 2}: Skipped because 'Subject Description' was empty.\")\n        continue\n\n    cats = []\n    for idx, col in enumerate(category_cols, start=1):\n        val = row.get(col, None)\n        name = str(val).strip() if pd.notna(val) else \"\"\n        if name:\n            cats.append({\"id\": idx, \"name\": name})\n\n    nb_research_dev = safe_int(row.get(\"number of research variation\", 0))\n    max_results = safe_int(row.get(\"number of url to check per research\", 0))\n    max_final_results = safe_int(row.get(\"number of desired results\", 0))\n    start_val = row.get(\"start research date\", None)\n    end_val = row.get(\"end research\", None)\n\n    subjects.append({\n        \"topic\": topic,\n        \"categories\": cats,\n        \"nb of research deviations\": nb_research_dev,\n        \"max_results\": max_results,\n        \"max_final_results\": max_final_results,\n        \"nb of results\": 0,\n        \"date research start\": to_iso_z(start_val),\n        \"date research end\": to_iso_z(end_val),\n    })\n\npayload = {\"subjects\": subjects}\njson_str = json.dumps(payload, ensure_ascii=False, indent=2)\n\noutput_path = \"/kaggle/working/subjects.json\"\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(json_str)\n\nprint(\"--- Generated JSON ---\")\nprint(json_str)\nprint(f\"\\n‚úÖ Successfully saved JSON to {output_path}\")\n\nif skipped_rows_log:\n    print(\"\\n--- Validation Summary ---\")\n    for log_entry in skipped_rows_log:\n        print(log_entry)\nelse:\n    print(\"\\n--- Validation Summary ---\")\n    print(\"All processed rows were valid.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:09.938399Z","iopub.execute_input":"2025-11-03T09:05:09.938658Z","iopub.status.idle":"2025-11-03T09:05:09.956901Z","shell.execute_reply.started":"2025-11-03T09:05:09.938624Z","shell.execute_reply":"2025-11-03T09:05:09.956166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Get settings ---\nfrom datetime import datetime, timedelta, time\nimport sys\n\nworksheet = spreadsheet.get_worksheet(1)\ndata = worksheet.get_all_records()\ndf = pd.DataFrame(data)\n\n# Read settings\ntry:\n    RECURENCE = int(str(df['Recurence'][0]).strip())\nexcept Exception:\n    raise ValueError(\"Recurence must be an integer number of days.\")\n\nNEXT_NEWSLETTER_DATE_STR = str(df['Next date'][0]).strip()\nEXCLUDED_DAYS_STR = str(df.get('Excluded days', [''])[0]).strip()\n\ndef parse_ddmmyyyy_or_y(date_str: str) -> datetime:\n    \"\"\"Parse DD/MM/YYYY or DD/MM/YY to a datetime (00:00).\"\"\"\n    for fmt in (\"%d/%m/%Y\", \"%d/%m/%y\"):\n        try:\n            return datetime.strptime(date_str, fmt)\n        except ValueError:\n            continue\n    # Last resort: pandas\n    try:\n        import pandas as pd\n        dt = pd.to_datetime(date_str, dayfirst=True, errors=\"raise\")\n        return datetime(dt.year, dt.month, dt.day)\n    except Exception:\n        raise ValueError(f\"Next date '{date_str}' must be in format DD/MM/YYYY or DD/MM/YY.\")\n\n# Parse next date (sheet stores DD/MM/YYYY or DD/MM/YY)\nnext_date = parse_ddmmyyyy_or_y(NEXT_NEWSLETTER_DATE_STR)\n\n# Excluded weekdays\nweekday_map = {\n    'monday': 0, 'tuesday': 1, 'wednesday': 2, 'thursday': 3,\n    'friday': 4, 'saturday': 5, 'sunday': 6\n}\nexcluded_day_names = [d.strip().lower() for d in EXCLUDED_DAYS_STR.split(',') if d.strip()]\nexcluded_weekdays = {weekday_map[d] for d in excluded_day_names if d in weekday_map}\n\ntoday = datetime.now().date()\n\n# ---- Recurrence roll-forward logic ----\n# Keep adding recurrence until the date is >= today.\nwhile next_date.date() < today:\n    next_date += timedelta(days=RECURENCE)\n\n# If we landed in the future, only then skip excluded weekdays (do NOT skip if it's exactly today).\nif next_date.date() > today:\n    while next_date.weekday() in excluded_weekdays:\n        next_date += timedelta(days=1)\n\n# Update the \"Next date\" in the Google Sheet to the adjusted value (DD/MM/YYYY)\nheaders = worksheet.row_values(1)\ntry:\n    col_next_date = headers.index('Next date') + 1\nexcept ValueError:\n    raise ValueError(\"Column 'Next date' not found in settings sheet header row.\")\n\nnew_date_str = next_date.strftime('%d/%m/%Y')\nworksheet.update_cell(2, col_next_date, new_date_str)\n\n# Continue only if the (possibly adjusted) date is today\nif next_date.date() != today:\n    print(f\"\\nüî¥ Newsletter date ({new_date_str}) is not today ({today.strftime('%d/%m/%Y')}).\")\n    print(\"Stopping Kaggle notebook execution.\")\n    sys.exit()\nelse:\n    print(f\"\\nüü¢ Newsletter date is today ({new_date_str}). Continuing‚Ä¶\")\n    NEXT_NEWSLETTER_DATE = next_date\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:09.957530Z","iopub.execute_input":"2025-11-03T09:05:09.957730Z","iopub.status.idle":"2025-11-03T09:05:13.013882Z","shell.execute_reply.started":"2025-11-03T09:05:09.957714Z","shell.execute_reply":"2025-11-03T09:05:13.013280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"worksheet = spreadsheet.get_worksheet(2) # To get the third tab\n\n# --- Fetch all data from the worksheet ---\nprint(\"Fetching all records from the worksheet...\")\ndata = worksheet.get_all_records()\n\n# --- Convert to a pandas DataFrame ---\nif data:\n    links_df = pd.DataFrame(data)\n    print(\"\\nSuccessfully fetched data. Here's a preview:\")\n    print(links_df.head()) # Display the first 5 rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:13.014543Z","iopub.execute_input":"2025-11-03T09:05:13.014758Z","iopub.status.idle":"2025-11-03T09:05:14.429230Z","shell.execute_reply.started":"2025-11-03T09:05:13.014740Z","shell.execute_reply":"2025-11-03T09:05:14.428489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --no-cache-dir --force-reinstall \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 \\\n  \"llama-cpp-python>=0.3.8\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:05:14.429995Z","iopub.execute_input":"2025-11-03T09:05:14.430307Z","iopub.status.idle":"2025-11-03T09:06:22.900342Z","shell.execute_reply.started":"2025-11-03T09:05:14.430286Z","shell.execute_reply":"2025-11-03T09:06:22.899266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# install the maintained client\n!pip -q install -U ddgs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:06:22.901950Z","iopub.execute_input":"2025-11-03T09:06:22.902318Z","iopub.status.idle":"2025-11-03T09:06:29.387934Z","shell.execute_reply.started":"2025-11-03T09:06:22.902276Z","shell.execute_reply":"2025-11-03T09:06:29.387078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------\n# 2.5Ô∏è‚É£  Llama.cpp (Gemma-3) GPU init for DeepSearch\n#       - uses both T4s with tensor_split\n#       - deterministic generation for JSON-only outputs\n# -------------------------------------------\nimport os\nfrom llama_cpp import Llama\nimport json\n\n# Use cuBLAS path if available (set BEFORE importing/instantiating in a fresh kernel)\nos.environ.setdefault(\"GGML_CUDA_FORCE_MMQ\", \"0\")\nos.environ.setdefault(\"GGML_CUDA_FORCE_CUBLAS\", \"1\")\nos.environ.setdefault(\"LLAMA_LOG_LEVEL\", \"WARN\")   # set to INFO for detailed logs\n# Target both T4s (optional)\nos.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0,1\")\n\nLLAMA_MODEL_PATH = \"/kaggle/input/gemma-3/gguf/gemma-3-12b-it-qat-q4_0/3/gemma-3-12b-it-q4_0.gguf\"\n\n# Create a single global instance reused by the whole pipeline\nllm = Llama(\n    model_path=LLAMA_MODEL_PATH,\n    n_gpu_layers=-1,\n    tensor_split=[0.5, 0.5],\n    main_gpu=0,\n    offload_kqv=True,\n    n_ctx=4096,          # ‚¨ÖÔ∏è bump from 2048 ‚Üí 4096 (or 6144/8192 if you have VRAM)\n    n_batch=256,         # ‚¨ÖÔ∏è speeds prompt ingestion (lower if you OOM)\n    verbose=False,\n)\n\n# Small helper to enforce \"JSON only\" and deterministic decoding\ndef _chat_json_only(messages, max_new_tokens=512):\n    \"\"\"\n    messages: List[{'role': 'system'|'user'|'assistant', 'content': str}]\n    returns a str (model content)\n    \"\"\"\n    # Make sure there is a strong system instruction\n    sys_prefix = {\"role\": \"system\", \"content\": \"You only output JSON ‚Äî no extra text.\"}\n    msgs = [sys_prefix] + [m for m in messages if m.get(\"role\") != \"system\"]\n\n    out = llm.create_chat_completion(\n        messages=msgs,\n        temperature=0.0,\n        top_p=1.0,\n        seed=12345,                # deterministic\n        max_tokens=max_new_tokens,\n        # stop can be omitted; JSON extraction below is robust\n    )\n    return out[\"choices\"][0][\"message\"][\"content\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:06:29.390886Z","iopub.execute_input":"2025-11-03T09:06:29.391190Z","iopub.status.idle":"2025-11-03T09:07:22.557282Z","shell.execute_reply.started":"2025-11-03T09:06:29.391160Z","shell.execute_reply":"2025-11-03T09:07:22.556705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------\n# 2Ô∏è‚É£ Subjects (Structured Metadata)\n#    - User provides dates manually (we DO NOT overwrite them)\n#    - \"nb of results\" = **max_results** used for DDG (per subject)\n#    - \"categories\" = list of categories to classify (ids + labels)\n#    - \"max_final_results\" = max items to keep per subject, sorted by relevance\n\nwith open(\"/kaggle/working/subjects.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# If it's a dict with numeric keys, convert values to list\nif isinstance(data, dict):\n    # Sort keys numerically if possible\n    try:\n        subjects = [v for k, v in sorted(data.items(), key=lambda x: int(x[0]))]\n    except Exception:\n        subjects = list(data.values())\nelse:\n    subjects = data\n\nif isinstance(subjects, list) and len(subjects) == 1 and isinstance(subjects[0], list):\n    subjects = subjects[0]\n\n# If it's a dict with numeric keys -> turn into a list (keeps numeric order)\nelif isinstance(subjects, dict):\n    try:\n        subjects = [v for k, v in sorted(subjects.items(), key=lambda kv: int(kv[0]))]\n    except Exception:\n        subjects = list(subjects.values())\n\n# If it's a list of JSON strings -> parse each string\nelif isinstance(subjects, list) and all(isinstance(x, str) for x in subjects):\n    subjects = [json.loads(x) for x in subjects]\n\nprint(f\"‚úÖ Normalized to {len(subjects)} subjects. Example keys:\", list(subjects[0].keys()))\n\n\n# -------------------------------------------\n# 3Ô∏è‚É£ Imports & Utility Functions\n# -------------------------------------------\nimport json, re, time, gc\nfrom typing import Any, Dict, List, Optional\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom ddgs import DDGS\nimport torch\n\n# NOTE: These are assumed to be available elsewhere in the notebook:\n# - tok, textgen, eos_id (for chat template inference)\n# - torch (for cuda empty cache)\n# - DDGS from duckduckgo_search (for web search)\n\n# ---- Web metadata fetch helpers ----\nimport requests, urllib.parse as urlparse\nfrom bs4 import BeautifulSoup\n\ndef _abs_url(base, href):\n    try:\n        return urlparse.urljoin(base, href)\n    except Exception:\n        return None\n\ndef _same_domain(u1, u2):\n    try:\n        return urlparse.urlparse(u1).netloc == urlparse.urlparse(u2).netloc\n    except Exception:\n        return False\n\ndef fetch_page_metadata(url: str, max_links: int = 6, timeout: float = 8.0) -> dict:\n    \"\"\"\n    Returns a compact dict describing the page, or {} on failure.\n    Avoids heavy downloads; times out quickly; strips long text.\n    \"\"\"\n    try:\n        resp = requests.get(\n            url,\n            headers={\"User-Agent\": \"Mozilla/5.0 (compatible; DeepSearch/1.0)\"},\n            timeout=timeout,\n            allow_redirects=True,\n        )\n    except Exception:\n        return {}\n\n    ctype = resp.headers.get(\"Content-Type\", \"\")\n    if \"html\" not in ctype.lower():\n        return {}\n\n    html = resp.text\n    soup = BeautifulSoup(html, \"html.parser\")\n\n    # Title via <title> or og:title\n    title = (soup.title.string or \"\").strip() if soup.title and soup.title.string else \"\"\n    og_title = soup.find(\"meta\", property=\"og:title\")\n    if og_title and og_title.get(\"content\"):\n        title = og_title.get(\"content\").strip() or title\n\n    # Description via og:description, meta[name=description], or first <p>\n    description = \"\"\n    og_desc = soup.find(\"meta\", property=\"og:description\")\n    meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n    if og_desc and og_desc.get(\"content\"):\n        description = og_desc.get(\"content\").strip()\n    elif meta_desc and meta_desc.get(\"content\"):\n        description = meta_desc.get(\"content\").strip()\n    if not description:\n        first_p = soup.find(\"p\")\n        if first_p:\n            description = \" \".join(first_p.get_text(\" \").split())\n\n    # Canonical URL\n    canonical = soup.find(\"link\", rel=lambda v: v and \"canonical\" in v)\n    canonical_url = canonical.get(\"href\").strip() if canonical and canonical.get(\"href\") else \"\"\n\n    # One H1 as a hint\n    h1 = soup.find(\"h1\")\n    h1_text = \" \".join(h1.get_text(\" \").split()) if h1 else \"\"\n\n    # A few internal links (href, text)\n    all_links, seen = [], set()\n    for a in soup.find_all(\"a\", href=True):\n        href = _abs_url(resp.url, a.get(\"href\"))\n        if not href or href in seen:\n            continue\n        seen.add(href)\n        if _same_domain(resp.url, href):\n            text = \" \".join((a.get_text(\" \") or \"\").split())\n            if text:\n                all_links.append({\"href\": href, \"text\": text[:120]})\n        if len(all_links) >= max_links:\n            break\n\n    # --- NEW: try to extract publish/modified timestamps from common tags ---\n    # --- Safe meta getter: always route through attrs=... to avoid name= collision ---\n    def _meta_content(**attrs):\n        tag = soup.find(\"meta\", attrs=attrs)\n        return tag.get(\"content\").strip() if tag and tag.get(\"content\") else \"\"\n\n    published_raw = (\n        _meta_content(property=\"article:published_time\")\n        or _meta_content(name=\"pubdate\")\n        or _meta_content(name=\"publish-date\")\n        or _meta_content(name=\"date\")\n        or _meta_content(itemprop=\"datePublished\")\n    )\n    \n    modified_raw = (\n        _meta_content(property=\"article:modified_time\")\n        or _meta_content(name=\"lastmod\")\n        or _meta_content(itemprop=\"dateModified\")\n    )\n    \n    # Also check <time datetime=\"...\">\n    if not published_raw:\n        t = soup.find(\"time\", attrs={\"datetime\": True})\n        if t and t.get(\"datetime\"):\n            published_raw = t.get(\"datetime\").strip()\n\n    def _trim(s, n=240):\n        return (s[:n] + \"‚Ä¶\") if s and len(s) > n else s\n\n    # Parse dates to ISO strings if possible (actual parsing helpers added below)\n    published_at_dt = _to_utc(_parse_dt(published_raw)) if published_raw else None\n    modified_at_dt  = _to_utc(_parse_dt(modified_raw))  if modified_raw  else None\n\n    return {\n        \"url\": resp.url or url,\n        \"canonical\": canonical_url or \"\",\n        \"title\": _trim(title),\n        \"h1\": _trim(h1_text),\n        \"description\": _trim(description, 320),\n        \"links\": all_links,\n        \"published_at\": published_at_dt.isoformat() if published_at_dt else \"\",\n        \"modified_at\": modified_at_dt.isoformat() if modified_at_dt else \"\",\n    }\n\n# ---- Search & JSON parsing helpers (from original cell) ----\ndef parse_json_list(text: str) -> Optional[List[str]]:\n    if not text:\n        return None\n    try:\n        data = json.loads(text)\n        if isinstance(data, list) and all(isinstance(x, str) for x in data):\n            return data\n    except Exception:\n        pass\n    m = re.search(r\"\\[.*?\\]\", text, flags=re.DOTALL)\n    if m:\n        try:\n            data = json.loads(m.group(0))\n            if isinstance(data, list) and all(isinstance(x, str) for x in data):\n                return data\n        except Exception:\n            return None\n    return None\n\n# ---\n# --- FIX 1: Make parser handle both lists [...] and objects {...}\n# ---\ndef extract_json_object(text: str) -> Optional[Any]: # Return Any (dict or list)\n    if not text:\n        return None\n    txt = text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n    \n    # 1. Try to parse the whole string directly\n    try:\n        data = json.loads(txt)\n        if isinstance(data, (dict, list)):\n            return data\n    except Exception:\n        pass # Not a clean JSON, fallback to regex\n\n    # 2. Fallback: Search for the first '[' or '{'\n    txt_clean = txt.strip()\n    first_char = \"\"\n    for char in txt_clean:\n        if char in ['{', '[']:\n            first_char = char\n            break\n            \n    m = None\n    if first_char == '{':\n        m = re.search(r\"\\{.*\\}\", txt_clean, flags=re.DOTALL)\n    elif first_char == '[':\n        m = re.search(r\"\\[.*\\]\", txt_clean, flags=re.DOTALL)\n\n    if m:\n        try:\n            # Try parsing the extracted regex block\n            data = json.loads(m.group(0))\n            if isinstance(data, (dict, list)): \n                 return data # Return the parsed data (could be dict or list)\n        except Exception:\n            return None # Failed to parse regex match\n            \n    return None # Return None if everything fails\n\n# -------------------------------------------\n# 4Ô∏è‚É£ Chat Template Wrapper (Gemma-safe via llama.cpp)\n# -------------------------------------------\ndef _to_parts(content):\n    \"\"\"Normalize content to a list of {type: 'text', text: '...'} parts.\"\"\"\n    if isinstance(content, str):\n        return [{\"type\": \"text\", \"text\": content}]\n    if isinstance(content, list):\n        if all(isinstance(x, dict) and \"type\" in x and \"text\" in x for x in content):\n            return content\n        if all(isinstance(x, str) for x in content):\n            return [{\"type\": \"text\", \"text\": x} for x in content]\n    return [{\"type\": \"text\", \"text\": str(content)}]\n\ndef chat_generate_json(messages, max_new_tokens: int = 512) -> str:\n    \"\"\"\n    Deterministic JSON-only generation using llama.cpp chat completion.\n    Ignores tok/textgen/eos_id; uses the GGUF-embedded chat template.\n    \"\"\"\n    # Keep your message normalization (so upstream calls don't change)\n    norm_messages = [{\"role\": m[\"role\"], \"content\": _to_parts(m.get(\"content\", \"\"))} for m in messages]\n\n    # Flatten parts back to plain strings for llama.cpp\n    flat = []\n    for m in norm_messages:\n        parts = m[\"content\"]\n        text = \"\\n\".join(p.get(\"text\", \"\") for p in parts if isinstance(p, dict))\n        flat.append({\"role\": m[\"role\"], \"content\": text})\n\n    return _chat_json_only(flat, max_new_tokens=max_new_tokens)\n\n# -------------------------------------------\n# 5Ô∏è‚É£ Date helpers + DDG with date filter\n# -------------------------------------------\nfrom datetime import datetime, timezone, timedelta\ntry:\n    from dateutil import parser as dtparse  # more robust parsing if available\nexcept Exception:\n    dtparse = None\n\ndef _parse_dt(s: str) -> Optional[datetime]:\n    if not s:\n        return None\n    s = s.strip().replace(\"Z\", \"+00:00\")\n    try:\n        if dtparse:\n            return dtparse.parse(s)\n        return datetime.fromisoformat(s)\n    except Exception:\n        return None\n\ndef _to_utc(dt: Optional[datetime]) -> Optional[datetime]:\n    if not dt:\n        return None\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\ndef _pick_timelimit_for_range(start_utc: datetime, end_utc: datetime) -> Optional[str]:\n    \"\"\"Map an exact range to DDG timelimit granularity when it fits neatly.\"\"\"\n    delta = end_utc - start_utc\n    if delta <= timedelta(days=1, seconds=1):\n        return \"d\"  # last day\n    if delta <= timedelta(days=7, seconds=1):\n        return \"w\"  # last week\n    if delta <= timedelta(days=31, seconds=1):\n        return \"m\"  # last month\n    if delta <= timedelta(days=365, seconds=1):\n        return \"y\"  # last year\n    return None\n\ndef _within_range(dt_val: Optional[datetime], start_utc: Optional[datetime], end_utc: Optional[datetime]) -> bool:\n    if not dt_val:\n        return False\n    dt_val = _to_utc(dt_val)\n    if start_utc and dt_val < start_utc:\n        return False\n    if end_utc and dt_val > end_utc:\n        return False\n    return True\n\ndef search_ddg(\n    subject_obj: Dict[str, Any],\n    query: str,\n    max_results: int = 20,\n    date_start_iso: Optional[str] = None,\n    date_end_iso: Optional[str] = None,\n):\n    \"\"\"\n    DuckDuckGo search with an in-query date filter only.\n    We map the requested window length to DDG's relative timelimit {d,w,m,y}.\n    NOTE: DDG timelimit is 'last day/week/month/year' (relative to now), not an absolute date range.\n    \"\"\"\n    # Compute timelimit from the requested span, if both dates provided\n    timelimit = None\n    if date_start_iso and date_end_iso:\n        start_utc = _to_utc(_parse_dt(date_start_iso))\n        end_utc   = _to_utc(_parse_dt(date_end_iso))\n        if start_utc and end_utc and end_utc >= start_utc:\n            timelimit = _pick_timelimit_for_range(start_utc, end_utc)  # -> 'd' | 'w' | 'm' | 'y' | None\n\n    # Execute DDG with timelimit hint only (no post-filtering)\n    try:\n        with DDGS() as ddgs:\n            kwargs = dict(max_results=max_results)\n            if timelimit:\n                kwargs[\"timelimit\"] = timelimit  # {'d','w','m','y'}\n            results = list(ddgs.text(query, **kwargs))\n    except Exception as e:\n        print(f\"[Search Error] '{query}' (topic='{subject_obj.get('topic','?')}'): {e}\")\n        return []\n\n    # Return URLs as-is (no extra filtering)\n    return [r.get(\"href\") for r in results if r.get(\"href\")]\n\n\n# -------------------------------------------\n# 6Ô∏è‚É£ Rephrase & Classify (using PAGE METADATA)\n# -------------------------------------------\ndef rephrase_subject(subject_text: str, n_variants: int = 10) -> List[str]:\n    messages = [\n        {\"role\": \"system\", \"content\": \"You only output JSON‚Äîno extra text.\"},\n        {\"role\": \"user\", \"content\": (\n            f\"Rephrase this into {n_variants} search queries under 12 words.\"\n            \"Return ONLY a JSON list of strings.\"\n            f\"Subject: {subject_text}JSON:\"\n        )}\n    ]\n    raw = chat_generate_json(messages, max_new_tokens=384)\n    return parse_json_list(raw) or [subject_text]\n\ndef classify_pages(pages: List[Dict[str, Any]], subject_obj: Dict[str, Any], debug: bool = False) -> Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Classify pages (with metadata) instead of bare URLs.\n    Returns {url: {\"category\": int, \"reason\": str, \"relevance\": int}}\n    - Enforces allowed category IDs by instruction; non-allowed items should be omitted by the model.\n    - Post-filter still drops anything outside allowed_ids to be safe.\n    - Robust JSON parsing (cleans trailing commas, code fences; accepts dict or list forms).\n    \"\"\"\n    if not pages:\n        return {}\n\n    subject_text = subject_obj.get(\"topic\", \"\")\n    categories   = subject_obj.get(\"categories\", []) or []\n    allowed_ids  = {str(c[\"id\"]) for c in categories} if categories else {\"1\", \"2\", \"3\"}\n    allowed_ids_str = \", \".join(sorted(allowed_ids))\n    cats_line = \", \".join(f\"{c['id']}={c['name']}\" for c in categories) if categories else (\n        \"1=Dashboard (interactive visualization), 2=Paid marketplace, 3=Downloadable dataset (CSV/JSON/ZIP)\"\n    )\n\n    # ---- Compact payload to keep prompts small\n    # - cap lengths\n    # - take at most a few internal links per page\n    # - drop entries with empty URL\n    compact_pages = []\n    for p in pages:\n        u = (p.get(\"url\") or \"\").strip()\n        if not u:\n            continue\n        links = p.get(\"links\") or []\n        links_small = []\n        for l in links[:6]:  # cap to 6 internal links\n            href = (l.get(\"href\") or \"\")[:200]\n            text = (l.get(\"text\") or \"\")[:80]\n            if href:\n                links_small.append({\"href\": href, \"text\": text})\n        compact_pages.append({\n            \"url\": u,\n            \"title\": (p.get(\"title\", \"\") or \"\")[:160],\n            \"desc\": (p.get(\"description\", \"\") or \"\")[:220],\n            \"h1\":   (p.get(\"h1\", \"\") or \"\")[:120],\n            \"links\": links_small,\n        })\n\n    # Nothing to classify\n    if not compact_pages:\n        return {}\n\n    payload_json = json.dumps(compact_pages, ensure_ascii=False)\n\n    # ---- Strong instruction to restrict to allowed IDs and omit others\n    messages = [\n        {\"role\": \"system\", \"content\": \"You only output JSON‚Äîno extra text.\"},\n        {\"role\": \"user\", \"content\": (\n            \"Classify these PAGES (with metadata) about: \"\n            f\"'{subject_text}'. \"\n            f\"Categories: {cats_line}. \"\n            f\"Allowed category IDs: [{allowed_ids_str}]. \"\n            \"USE ONLY the allowed IDs. If a page does NOT match an allowed category, OMIT it entirely (do not include the URL). \"\n            \"Output ONE JSON object ONLY (no wrapper keys). \"\n            'Keys MUST be the page \"url\" values. '\n            'Values MUST be: {\"category\": <allowed id>, \"reason\":\"short\", \"relevance\": 1-5 (int, 5=high)}. '\n            f\"Pages JSON: {payload_json}\\n\"\n            \"JSON only:\"\n        )}\n    ]\n\n    # Generate JSON (deterministic)\n    raw = chat_generate_json(messages, max_new_tokens=1024)\n    if debug:\n        preview = raw[:500] + (\"‚Ä¶\" if len(raw) > 500 else \"\")\n        print(\"üß™ Raw model JSON:\", preview)\n\n    # --- Forgiving parse (accept dict OR list; cleanup trailing commas / code fences)\n    def _cleanup_json_maybe(text: str) -> str:\n        if not text:\n            return text\n        t = text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n        # remove trailing commas before ] or }\n        t = re.sub(r\",(\\s*[\\]\\}])\", r\"\\1\", t)\n        return t\n\n    def _extract_obj_or_list(text: str) -> Optional[Any]:\n        if not text:\n            return None\n        t = _cleanup_json_maybe(text)\n        # 1) try whole string\n        try:\n            data = json.loads(t)\n            if isinstance(data, (dict, list)):\n                return data\n        except Exception:\n            pass\n        # 2) find first JSON block\n        first = next((c for c in t if c in \"{}[]\"), \"\")\n        if not first:\n            return None\n        patt = r\"\\{.*\\}\" if first == \"{\" else r\"\\[.*\\]\"\n        m = re.search(patt, t, flags=re.DOTALL)\n        if m:\n            try:\n                block = _cleanup_json_maybe(m.group(0))\n                data = json.loads(block)\n                if isinstance(data, (dict, list)):\n                    return data\n            except Exception:\n                return None\n        return None\n\n    parsed = _extract_obj_or_list(raw) or {}\n\n    # Normalize common shapes: list-of-objects or {\"URLs\": {...}}\n    if isinstance(parsed, dict) and \"URLs\" in parsed and isinstance(parsed[\"URLs\"], dict):\n        parsed = parsed[\"URLs\"]\n    elif isinstance(parsed, list):\n        merged = {}\n        for item in parsed:\n            if isinstance(item, dict) and \"url\" in item:\n                url_key = item.get(\"url\")\n                if url_key:\n                    merged[url_key] = {\n                        \"category\": item.get(\"category\"),\n                        \"reason\": item.get(\"reason\", \"\"),\n                        \"relevance\": item.get(\"relevance\", 1),\n                    }\n            elif isinstance(item, dict) and len(item) == 1:\n                # shape: [{\"https://‚Ä¶\": {...}}, {\"https://‚Ä¶\": {...}}]\n                merged.update(item)\n        parsed = merged\n\n    if not isinstance(parsed, dict):\n        if debug:\n            print(\"‚ö†Ô∏è Parsed JSON is not an object after normalization. Got:\", type(parsed).__name__)\n        return {}\n\n    # ---- Final sanitize + enforce allowed_ids\n    out: Dict[str, Dict[str, Any]] = {}\n    dropped = {\"not_dict\": 0, \"no_category\": 0, \"bad_category\": 0}\n\n    for u, d in parsed.items():\n        if not isinstance(d, dict):\n            dropped[\"not_dict\"] += 1\n            continue\n\n        # category ‚Üí normalize to string int\n        cat = d.get(\"category\")\n        try:\n            cat_str = str(int(cat))\n        except Exception:\n            m = re.search(r\"\\d+\", str(cat) if cat is not None else \"\")\n            cat_str = m.group(0) if m else None\n\n        if not cat_str:\n            dropped[\"no_category\"] += 1\n            continue\n        if cat_str not in allowed_ids:\n            dropped[\"bad_category\"] += 1\n            continue  # enforce allowed IDs\n\n        # relevance ‚Üí int in [1..5]\n        rel = d.get(\"relevance\", 1)\n        try:\n            relevance = int(rel)\n            if relevance < 1:\n                relevance = 1\n            elif relevance > 5:\n                relevance = 5\n        except Exception:\n            relevance = 1\n\n        # reason (short)\n        reason = (d.get(\"reason\", \"\") or \"\")[:180]\n\n        out[u] = {\"category\": int(cat_str), \"reason\": reason, \"relevance\": relevance}\n\n    if debug:\n        print(f\"‚úÖ Kept {len(out)} items (dropped: {dropped}) | allowed_ids={sorted(allowed_ids)}\")\n    return out\n\n\n# -------------------------------------------\n# 7Ô∏è‚É£ DeepSearch Main (metadata + date-aware DDG ‚Üí page-level classification)\n#    - Dates remain whatever user set; NOT modified here\n# -------------------------------------------\nresults: List[Dict[str, Any]] = []\nprint(\"üöÄ Starting Gemma 3 DeepSearch‚Ä¶\")\n\nfor subject in tqdm(subjects, desc=\"üìå Subjects\", unit=\"subject\"):\n    topic = subject[\"topic\"]\n    n_variants = subject[\"nb of research deviations\"]\n    mr = subject.get(\"max_results\", 20)\n\n    print(f\"\\nüîç Processing subject: {topic}\")\n    print(f\"   ‚Üí Variants: {n_variants}, Max results per query: {mr}\")\n\n    # Rephrase subject\n    print(\"‚úèÔ∏è Rephrasing queries...\")\n    queries = rephrase_subject(topic, n_variants)\n    print(f\"   ‚Üí Generated {len(queries)} rephrased queries\")\n\n    # Gather URLs via DDG (DATE-AWARE)\n    all_urls, query_map = set(), {}\n\n    print(\"üåê Running DuckDuckGo searches (date-aware)‚Ä¶\")\n    blacklist_urls = set(links_df['url'].dropna().astype(str))\n    for q in tqdm(queries, desc=\"   üîé Queries\", unit=\"query\"):\n        print(f\"     ‚Üí Searching for: '{q}'\")\n    \n        # We will fetch more results than needed to have backups.\n        # A factor of 3 is usually a safe bet.\n        fetch_limit = mr * 3\n    \n        potential_urls = search_ddg(\n            subject,\n            q,\n            max_results=fetch_limit, # Fetch a larger batch of URLs\n            date_start_iso=subject.get(\"date research start\"),\n            date_end_iso=subject.get(\"date research end\"),\n        )\n    \n        print(f\"       Found {len(potential_urls)} potential URLs to check against blacklist.\")\n    \n        new_urls_added_count = 0\n        for u in potential_urls:\n            # Stop once we have found enough valid URLs for this query\n            if new_urls_added_count >= mr:\n                break\n    \n            # Check if the URL is new AND not on the blacklist\n            if u not in all_urls and u not in blacklist_urls:\n                all_urls.add(u)\n                query_map[u] = q\n                new_urls_added_count += 1\n    \n        print(f\"       Added {new_urls_added_count} new, non-blacklisted URLs for this query.\")\n        time.sleep(0.4) # be polite\n\n    subject[\"nb of results\"] = mr\n    print(f\"üìä Total unique URLs gathered: {len(all_urls)}\")\n\n    if not all_urls:\n        print(\"‚ö†Ô∏è No URLs found for this subject, skipping‚Ä¶\")\n        continue\n\n    # Cleanup\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    # --- Fetch lightweight metadata for each URL ---\n    print(\"üß© Fetching page metadata‚Ä¶\")\n    unique_urls = list(all_urls)\n    page_meta_map: Dict[str, Dict[str, Any]] = {}\n    for u in tqdm(unique_urls, desc=\"   üåê Pages\", unit=\"page\"):\n        page_meta_map[u] = fetch_page_metadata(u)\n\n    # Classify using page metadata\n    print(\"üóÇÔ∏è Starting classification of pages...\")\n    all_classified = {}\n    batch_size = 5\n    for i in tqdm(range(0, len(unique_urls), batch_size), desc=\"   üì¶ Classification batches\", unit=\"batch\"):\n        chunk_urls = unique_urls[i:i+batch_size]\n        # Build chunk of pages with metadata\n        chunk_pages = []\n        for u in chunk_urls:\n            meta = page_meta_map.get(u, {}) or {}\n            chunk_pages.append({\n                \"url\": u,\n                \"title\": meta.get(\"title\", \"\"),\n                \"description\": meta.get(\"description\", \"\"),\n                \"h1\": meta.get(\"h1\", \"\"),\n                \"links\": meta.get(\"links\", []),\n            })\n        # Keep debug=True so you can see the output\n        chunk_res = classify_pages(chunk_pages, subject, debug=True) \n        all_classified.update(chunk_res)\n        time.sleep(1)\n\n    # Aggregate\n    for url, d in all_classified.items():\n        results.append({\n            \"subject\": topic,\n            \"query\": query_map.get(url, \"\"),\n            \"url\": url,\n            \"class\": d[\"category\"],\n            \"reason\": d[\"reason\"],\n            \"relevance\": d.get(\"relevance\", 1), # <-- NEW: Add relevance score\n        })\n\n    print(f\"‚úÖ Finished subject: {topic} ({len(all_classified)} classified URLs)\")\n\n# --- MODIFIED: Replaced the entire saving block ---\n\n# -------------------------------------------\n# 8Ô∏è‚É£ Filter, Rank, and Save Results\n# -------------------------------------------\nprint(\"\\nüíæ Filtering and saving results...\")\nif results:\n    # --- NEW: Filter to top X per subject based on relevance ---\n    print(f\"üî¨ Found {len(results)} total classified items. Filtering to top-k per subject...\")\n\n    # 1. Create the mapping from subject topic -> max_final_results\n    topic_to_max_final = {\n        s['topic']: s.get('max_final_results', 10) for s in subjects # Default 10\n    }\n\n    # 2. Convert all results to DataFrame\n    df = pd.DataFrame(results)\n    final_dfs = []\n\n    # 3. Group by subject, sort by relevance, and take the top-k\n    for subject_topic, group_df in df.groupby('subject'):\n        limit = topic_to_max_final.get(subject_topic, 10) # Get k\n        \n        # Sort by relevance (desc) and take the top 'limit' rows\n        top_group_df = group_df.sort_values(by='relevance', ascending=False).head(limit)\n        final_dfs.append(top_group_df)\n\n    # 4. Combine the filtered groups back into one DataFrame\n    if final_dfs:\n        final_df = pd.concat(final_dfs).sort_index() # sort_index to restore original-ish order\n        # Convert the final DF back to a list of dicts for JSON saving\n        final_results_list = final_df.to_dict('records')\n        print(f\"‚úÖ Filtered down to {len(final_results_list)} items.\")\n    else:\n        print(\"‚ö†Ô∏è No items remained after grouping/filtering.\")\n        final_df = pd.DataFrame(columns=results[0].keys() if results else []) # empty DF\n        final_results_list = []\n\n    # 5. Save the *FILTERED* results\n    final_df.to_csv(\"deepsearch_results.csv\", index=False)\n    with open(\"deepsearch_results.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(final_results_list, f, indent=2, ensure_ascii=False)\n\n    # Save the subjects summary (which now includes 'max_final_results')\n    pd.DataFrame(subjects).to_csv(\"deepsearch_subjects_summary.csv\", index=False)\n    print(\"üéâ Done! Filtered results saved to deepsearch_results.csv / .json and deepsearch_subjects_summary.csv\")\nelse:\n    print(\"ü§∑ No relevant results found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:07:22.558255Z","iopub.execute_input":"2025-11-03T09:07:22.558682Z","iopub.status.idle":"2025-11-03T09:16:58.570916Z","shell.execute_reply.started":"2025-11-03T09:07:22.558641Z","shell.execute_reply":"2025-11-03T09:16:58.570270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\nimport pytz\nimport csv\n\nparis = pytz.timezone('Europe/Paris')\ntoday_str = datetime.now(paris).strftime('%d/%m/%Y')\n\n# ADD result to GSHEET\ndata_ws = worksheet.spreadsheet.get_worksheet(2)\n\n# Ensure headers exist on the 3rd tab (optional safety)\nexisting_header = data_ws.row_values(1)\nexpected_header = [\"Date\", \"subject\", \"query\", \"url\", \"class\", \"reason\", \"relevance\"]\nif not existing_header:\n    data_ws.insert_row(expected_header, index=1)\n\n# Read CSV and build rows with today's date in DD/MM/YYYY\nrows_to_append = []\ncsv_path = \"/kaggle/working/deepsearch_results.csv\"\n\nwith open(csv_path, newline='', encoding='utf-8') as f:\n    reader = csv.DictReader(f)\n    for r in reader:\n        rows_to_append.append([\n            today_str,\n            r.get('subject', ''),\n            r.get('query', ''),\n            r.get('url', ''),\n            # Coerce numeric-looking fields safely; leave blank if not present\n            int(r['class']) if (r.get('class') or '').isdigit() else r.get('class', ''),\n            r.get('reason', ''),\n            int(r['relevance']) if (r.get('relevance') or '').isdigit() else r.get('relevance', ''),\n        ])\n\n# Batch-append (fast + preserves existing data)\nif rows_to_append:\n    data_ws.append_rows(rows_to_append, value_input_option='USER_ENTERED')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:16:58.571721Z","iopub.execute_input":"2025-11-03T09:16:58.572376Z","iopub.status.idle":"2025-11-03T09:17:01.778391Z","shell.execute_reply.started":"2025-11-03T09:16:58.572357Z","shell.execute_reply":"2025-11-03T09:17:01.777579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom datetime import datetime\nimport os\nfrom urllib.parse import urlsplit\nfrom collections import defaultdict\n\n# --- Configuration ---\nTEMPLATE_HTML = \"/kaggle/input/template-pptx/newsletter_template.html\"\nJSON_DATA     = \"/kaggle/working/deepsearch_results.json\"\nOUTPUT_HTML   = f\"newsletter_{datetime.now().strftime('%Y-%m-%d')}.html\"\n\n# Placeholders from the template:\nPH_DATE      = \"<!-- DATE_PLACEHOLDER -->\"\nPH_SUMMARY   = \"<!-- SUMMARY_ITEMS_PLACEHOLDER -->\"\nPH_SECTIONS  = \"<!-- SUBJECT_SECTIONS_PLACEHOLDER -->\"\n\n# --- 1) Load JSON robustly (array JSON or NDJSON), avoid crashing on empty ---\ndef load_results(path):\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            raw = f.read().strip()\n            if not raw:\n                # Empty file -> return empty list\n                return []\n            # Try standard JSON first\n            try:\n                obj = json.loads(raw)\n                if isinstance(obj, list):\n                    return obj\n                # If it's a dict, try to find a list under a common key\n                for k in (\"results\", \"data\", \"items\"):\n                    if isinstance(obj.get(k, None), list):\n                        return obj[k]\n                # Fallback: wrap single object\n                return [obj]\n            except json.JSONDecodeError:\n                # Try NDJSON line-by-line\n                items = []\n                for line in raw.splitlines():\n                    line = line.strip()\n                    if not line:\n                        continue\n                    try:\n                        items.append(json.loads(line))\n                    except json.JSONDecodeError:\n                        # Skip bad lines rather than crashing\n                        continue\n                return items\n    except FileNotFoundError:\n        print(f\"Erreur : Le fichier JSON '{path}' est introuvable.\")\n        raise SystemExit(1)\n\ndata = load_results(JSON_DATA)\n\n# --- 2) Group by subject (keep all results) ---\nsubjects_data = defaultdict(list)\nfor item in data:\n    subject = (item or {}).get(\"subject\")\n    if not subject:\n        continue\n    subjects_data[subject].append({\n        \"url\": item.get(\"url\", \"N/A\"),\n        \"reason\": item.get(\"reason\", \"N/A\"),\n        \"class_id\": item.get(\"class\", 0),\n    })\nprint(f\"{len(subjects_data)} sujets trouv√©s dans le JSON.\")\n\n# --- 3A) Build the summary (max 5 subjects) ---\nsummary_parts = []\nfor i, subject in enumerate(list(subjects_data.keys())[:5]):\n    summary_parts.append(f\"\"\"\n    <tr>\n        <td style=\"padding-bottom: 10px;\">\n            <table align=\"center\">\n                <tr>\n                    <td align=\"center\" valign=\"middle\" style=\"padding:0;\">\n                      <div style=\"\n                          background-color:#e84379;\n                          border-radius:50%;\n                          width:30px;\n                          height:30px;\n                          display:inline-block;\n                          text-align:center;\n                          line-height:30px;\n                      \">\n                        <p style=\"\n                            color:#ffffff;\n                            font-size:16px;\n                            font-weight:bold;\n                            margin:0;\n                            line-height:30px;\n                        \">{i+1}</p>\n                      </div>\n                    </td>\n                    <td style=\"padding-left: 15px;\">\n                        <p style=\"color: #ffffff; font-size: 16px; margin: 0; text-align: left;\">{subject}</p>\n                    </td>\n                </tr>\n            </table>\n        </td>\n    </tr>\"\"\")\nsummary_html = \"\".join(summary_parts)\nprint(\"Sommaire HTML g√©n√©r√©.\")\n\n# --- 3B) Build subject sections ---\nsection_blocks = []\nfor i, (subject, links) in enumerate(subjects_data.items()):\n    bg_color = \"#ffffff\" if i % 2 == 0 else \"#F8F9FA\"\n\n    link_lines = []\n    for link in links:\n        url = link[\"url\"]\n        netloc = urlsplit(url).netloc if url and url != \"N/A\" else \"N/A\"\n        link_lines.append(f\"\"\"\n        <p style=\"font-size: 12px; color: #333333; margin-bottom: 15px; word-break: break-all;\">\n            - <a target=\"_blank\" href=\"{url}\" style=\"color: #0066cc;\">{netloc}</a> &rarr; {link['reason']}\n        </p>\"\"\")\n    links_html = \"\".join(link_lines)\n\n    section_blocks.append(f\"\"\"\n    <tr>\n        <td style=\"padding: 0 0 40px; background-color: {bg_color};\">\n            <img src=\"https://i.imgur.com/CLeuYYS.png\" alt=\"divider\" style=\"width:100%; max-width:600px; display: block; margin-bottom: 40px;\">\n            <div style=\"padding: 0 30px;\">\n                <p style=\"color: #888888; font-size: 14px; margin: 0;\">Newsletter list</p>\n                <p style=\"color: #1a1a1a; font-size: 24px; font-weight: bold; margin: 5px 0 20px;\">{subject}</p>\n                {links_html}\n            </div>\n        </td>\n    </tr>\"\"\")\nsections_html = \"\".join(section_blocks)\nprint(\"Sections de contenu HTML g√©n√©r√©es.\")\n\n# --- 4) Read template and substitute the 3 placeholders ---\ntry:\n    with open(TEMPLATE_HTML, \"r\", encoding=\"utf-8\") as f:\n        template_content = f.read()\nexcept FileNotFoundError:\n    print(f\"Erreur : Le fichier template '{TEMPLATE_HTML}' est introuvable.\")\n    raise SystemExit(1)\n\nfinal_html = (\n    template_content\n        .replace(PH_DATE, datetime.now().strftime(\"%d/%m/%Y\"))\n        .replace(PH_SUMMARY, summary_html)\n        .replace(PH_SECTIONS, sections_html)\n)\n\n# --- 5) Save the final HTML ---\nwith open(OUTPUT_HTML, \"w\", encoding=\"utf-8\") as f:\n    f.write(final_html)\n\nprint(f\"\\nNewsletter g√©n√©r√©e avec succ√®s ! Fichier sauvegard√© sous : '{os.path.abspath(OUTPUT_HTML)}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:17:01.779253Z","iopub.execute_input":"2025-11-03T09:17:01.779433Z","iopub.status.idle":"2025-11-03T09:17:01.805745Z","shell.execute_reply.started":"2025-11-03T09:17:01.779418Z","shell.execute_reply":"2025-11-03T09:17:01.804983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get email\nworksheet = spreadsheet.get_worksheet(1) # To get the third tab\n\n# --- Fetch all data from the worksheet ---\nprint(\"Fetching all records from the worksheet...\")\ndata = worksheet.get_all_records()\n\n\nemails_df = pd.DataFrame(data)\nemail_list = [email.strip() for email in emails_df['List of contacts'][0].split(',')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:17:01.806433Z","iopub.execute_input":"2025-11-03T09:17:01.806670Z","iopub.status.idle":"2025-11-03T09:17:04.887136Z","shell.execute_reply.started":"2025-11-03T09:17:01.806632Z","shell.execute_reply":"2025-11-03T09:17:04.886379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sib_api_v3_sdk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:17:04.887973Z","iopub.execute_input":"2025-11-03T09:17:04.888267Z","iopub.status.idle":"2025-11-03T09:17:11.582606Z","shell.execute_reply.started":"2025-11-03T09:17:04.888242Z","shell.execute_reply":"2025-11-03T09:17:11.581697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sib_api_v3_sdk\nfrom sib_api_v3_sdk.rest import ApiException\nimport os\nfrom datetime import datetime\n\n# --- 1. Configuration ---\n\n# Paste your Brevo API key here\nAPI_KEY = user_secrets.get_secret(\"BREVO\") \n\n# This is the HTML file you just created\n# It uses the variable from your previous script\nHTML_TO_SEND = OUTPUT_HTML # Or \"newsletter_2025-10-21.html\"\n\n# --- Email details ---\nSENDER_EMAIL = \"menardisaac@gmail.com\"\nSENDER_NAME = \"Newsletter MyTraffic Bot\"\nEMAIL_SUBJECT = f\"Your Newsletter - {datetime.now().strftime('%Y-%m-%d')}\"\n\nunique_emails = email_list\nprint(f\"Preparing to send email to {len(unique_emails)} unique recipients: {unique_emails}\")\n\n\n# --- 2. Read the HTML file content ---\ntry:\n    with open(HTML_TO_SEND, 'r', encoding='utf-8') as f:\n        html_content = f.read()\n    print(f\"Successfully read HTML file: {HTML_TO_SEND}\")\nexcept FileNotFoundError:\n    print(f\"ERROR: HTML file not found at {HTML_TO_SEND}\")\n    # exit() # Uncomment if you want to stop the script on error\nexcept Exception as e:\n    print(f\"ERROR: Could not read HTML file: {e}\")\n    # exit() # Uncomment if you want to stop the script on error\n\n# --- 3. Configure Brevo API ---\nconfiguration = sib_api_v3_sdk.Configuration()\nconfiguration.api_key['api-key'] = API_KEY\n\napi_instance = sib_api_v3_sdk.TransactionalEmailsApi(sib_api_v3_sdk.ApiClient(configuration))\n\n# --- 4. Create the Email Object ---\nsender = {\"name\": SENDER_NAME, \"email\": SENDER_EMAIL}\n\n# --- MODIFIED: Build the 'to' list for the API ---\n# The API expects a list of dictionaries, e.g., [{'email':'a@b.com'}, {'email':'c@d.com'}]\nto = [{\"email\": email} for email in unique_emails]\n\n\nsend_smtp_email = sib_api_v3_sdk.SendSmtpEmail(\n    to=to,\n    sender=sender,\n    subject=EMAIL_SUBJECT,\n    html_content=html_content\n)\n\n# --- 5. Send the Email ---\ntry:\n    api_response = api_instance.send_transac_email(send_smtp_email)\n    print(\"\\n‚úÖ Email sent successfully!\")\n    print(\"Brevo API Response:\")\n    print(api_response)\nexcept ApiException as e:\n    print(f\"\\n‚ùå Error sending email via Brevo: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:17:11.583716Z","iopub.execute_input":"2025-11-03T09:17:11.583961Z","iopub.status.idle":"2025-11-03T09:17:12.515876Z","shell.execute_reply.started":"2025-11-03T09:17:11.583938Z","shell.execute_reply":"2025-11-03T09:17:12.515176Z"}},"outputs":[],"execution_count":null}]}